[
# Create a collection with two indexes:
normal create database=xyz collection=c numberOfShards=3 replicationFactor=3 drop=true
normal createIdx database=xyz collection=c withGeo=false numberFields=1 idxName=one
normal createIdx database=xyz collection=c withGeo=false numberFields=2 idxName=two

# Insert a terabyte of data:
normal insert database=xyz collection=c size=1T documentSize=300 withGeo=false withWords=0 parallelism=64 numberFields=5

# Try random reads:
normal randomRead database=xyz collection=c parallelism=64 loadPerThread=300000
# Try bulk reads:
normal queryOnIdx database=xyz collection=c queryLimit=1000 parallelism=64 loadPerThread=1200 idxName=primary
# Try random index reads:
normal queryOnIdx database=xyz collection=c queryLimit=1 parallelism=64 loadPerThread=180000 idxName=one
# Try index reads:
normal queryOnIdx database=xyz collection=c queryLimit=1000 parallelism=64 loadPerThread=600 idxName=two

# Replace all data:
normal randomReplace database=xyz collection=c parallelism=64 loadPerThread=1200 batchSize=1000

# Update all data:
normal randomUpdate database=xyz collection=c parallelism=64 loadPerThread=1200 batchSize=1000

# Drop stuff again:
normal drop database=xyz collection=c
normal dropDatabase database=xyz

# Now two collections, half the data each:
normal create database=xyz collection=d numberOfShards=3 replicationFactor=3 drop=true
normal createIdx database=xyz collection=d withGeo=false numberFields=1 idxName=one
normal createIdx database=xyz collection=d withGeo=false numberFields=2 idxName=two
normal create database=xyz collection=e numberOfShards=3 replicationFactor=3 drop=true
normal createIdx database=xyz collection=e withGeo=false numberFields=1 idxName=one
normal createIdx database=xyz collection=e withGeo=false numberFields=2 idxName=two

# Insert data concurrently:
{
normal insert database=xyz collection=d size=500G documentSize=300 withGeo=false withWords=0 parallelism=64 numberFields=5
normal insert database=xyz collection=e size=500G documentSize=300 withGeo=false withWords=0 parallelism=64 numberFields=5
}

# Replace with concurrent random reads:
{
normal randomReplace database=xyz collection=d parallelism=64 batchSize=1000 loadPerThread=1200
normal randomRead database=xyz collection=e parallelism=64 loadPerThread=6000
}

# Update with concurrent random reads:
{
normal randomUpdate database=xyz collection=d parallelism=64 batchSize=1000 loadPerThread=1200
normal randomRead database=xyz collection=e parallelism=64 loadPerThread=6000
}

# And drop again:
normal drop database=xyz collection=d
normal drop database=xyz collection=e
normal dropDatabase database=xyz

]
